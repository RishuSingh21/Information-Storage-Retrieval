{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3e774508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XIE19960413.0003 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open (\"data//result.trectext\",'r') as f:\n",
    "    doc = f.readlines()\n",
    "    print(doc[50000])\n",
    "        \n",
    "\n",
    "# for i in range(0,len(doc_list)-1):\n",
    "#     print(\"------\")\n",
    "#     my_list = [doc_list[i],doc_list[i+1]]\n",
    "#     print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10b06d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8243eebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XIE19960101.0001',\n",
       " 'normal egypt activ support anti iraq coalit gulf crisi symbol visit egyptian presid hosni mubarak jordan januari time jordan improv tie palestinian dispel fear arous palestinian due jordan special role jerusalem jordanian isra accord addit jordan made effort push forward palestinian isra negoti expand palestinian autonomi west bank benefit jordan peac treati israel attitud baghdad brought warmer tie western countri unit state vice presid al gore british prime minist john major german chancellor helmut kohl japanes prime minist tomiichi murayama spanish prime minist felip gonzalez visit amman reiter countri support kingdom field secretari state warren christoph jordan time meet king hussein dozen shuttl visit region expedit ongo middl east peac process clinton administr voic full support reserv kingdom defend secur immedi iraqi industri minist kamel hassan defect jordan august warn threat iraqi retali jordan past year king hussein crown princ hassan prime minist sharif zeid ben shaker senior jordanian offici travel london pari tokyo bonn washington bring home agreement econom aid militari assist promis support success host middl east north africa econom summit amman drew particip govern offici privat businessmen countri world ad great credit kingdom consid crown accomplish jordan foreign polici']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c879d9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values(['p'])\n"
     ]
    }
   ],
   "source": [
    "with open(\"data//result.trectext\", 'r') as f:\n",
    "    doc_content = f.readlines()\n",
    "    \n",
    "term_index={}\n",
    "for i in range(0,5):\n",
    "    term = doc_content[i]\n",
    "    mystr=str(i)\n",
    "    term_index[i]={} \n",
    "    term_index[i][term]= \"p\"\n",
    "\n",
    "k = term_index[4].values()\n",
    "print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b30ca165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([3, 2, 45])\n"
     ]
    }
   ],
   "source": [
    "mydict={\"word\":{1:3,3:2,4:45},\"tail\":{6:5,3:2},\"cat\":{1:4,7:5,45:3}}\n",
    "k = mydict[\"word\"].values()\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9f5ac874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessedCorpusReader:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.doc = open(\"data//testresult.trectext\",'r')\n",
    "        return\n",
    "\n",
    "    # Read a line for docNo from the corpus, read another line for the content, and return them in [docNo, content].\n",
    "    def nextDocument(self):\n",
    "        doc_no = self.doc.readline().strip()\n",
    "        if(doc_no == \"\"):\n",
    "            self.doc.close()\n",
    "        else:\n",
    "            doc_content = self.doc.readline().strip()\n",
    "            return [doc_no,doc_content]\n",
    "\n",
    "import json\n",
    "\n",
    "# Efficiency and memory cost should be paid with extra attention.\n",
    "class MyIndexWriter:\n",
    "    doc_id = 1\n",
    "    term_index={}\n",
    "    doc_index={}\n",
    "    def __init__(self):\n",
    "        self.doc = open(\"data//testresult.trectext\", 'r')\n",
    "        return\n",
    "\n",
    "    # This method build index for each document.\n",
    "\t# NT: in your implementation of the index, you should transform your string docno into non-negative integer docids,\n",
    "    # and in MyIndexReader, you should be able to request the integer docid for each docno.\n",
    "    def index(self, docNo, content):\n",
    "        #creating index to keep a record of doc_id and docNo\n",
    "\n",
    "        self.doc_index[docNo] = self.doc_id\n",
    "        \n",
    "        #Creating term index {term:{doc_id:freq}}\n",
    "        doc_content = content.split(\" \")\n",
    "        for i in range(0,len(doc_content)):\n",
    "            \n",
    "            term = doc_content[i]\n",
    "            print(term)\n",
    "            if term in self.term_index.keys():\n",
    "                if self.doc_id in self.term_index[term].keys(): \n",
    "                    self.term_index[term][self.doc_id] += 1\n",
    "                else:\n",
    "                    self.term_index[term][self.doc_id] = 1   \n",
    "            else:    \n",
    "                self.term_index[term]={}\n",
    "                self.term_index[term][self.doc_id] = 1\n",
    "        self.doc_id += 1\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    # Close the index writer, and you should output all the buffered content (if any).\n",
    "    def close(self):\n",
    "        with open(\"guru.txt\",'w+') as file:\n",
    "            file.writelines(json.dumps(self.term_index))\n",
    "        with open(\"docindex.txt\",\"w+\") as file1:\n",
    "            file1.writelines(json.dumps(self.doc_index))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "34b03fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n",
      "egypt\n",
      "activ\n",
      "januari\n",
      "xinhua\n",
      "jordan\n",
      "past\n",
      "januari\n",
      "januari\n",
      "xinhua\n",
      "violenc\n",
      "rise\n",
      "egypt\n",
      "rise\n",
      "totally finish  3  docs\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "    # Initiate pre-processed collection file reader.\n",
    "corpus =PreprocessedCorpusReader()\n",
    "    # Initiate the index writer.\n",
    "indexWriter = MyIndexWriter()\n",
    "    # Build index of corpus document by document.\n",
    "while True:\n",
    "    doc = corpus.nextDocument()\n",
    "    if doc == None:\n",
    "        break\n",
    "    indexWriter.index(doc[0], doc[1])\n",
    "    count+=1\n",
    "    if count%30000==0:\n",
    "        print(\"finish \", count,\" docs\")\n",
    "print(\"totally finish \", count, \" docs\")\n",
    "indexWriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9eef9e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'XIE19960101.0001': 1, 'XIE19960101.0002': 2, 'XIE19960101.0003': 3}\n",
      "['XIE19960101.0002']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"docindex.txt\",'r') as file:\n",
    "    doc_index = file.read()\n",
    "    js = json.loads(doc_index)\n",
    "    print(js)\n",
    "    key = [k for k,v in js.items() if v == 2]\n",
    "    print(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0eb5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Efficiency and memory cost should be paid with extra attention.\n",
    "class MyIndexReader:\n",
    "\n",
    "    def __init__(self):\n",
    "        with open(\"guru.txt\",'r') as doc:\n",
    "            self.term_index = doc.read()\n",
    "            self.term_js = json.loads(self.term_index)\n",
    "\n",
    "    # Return the integer DocumentID of input string DocumentNo.\n",
    "    def getDocId(self, docNo):\n",
    "        with open(\"docindex.txt\",'r') as file:\n",
    "            doc_index = file.read()\n",
    "            js = json.loads(doc_index)\n",
    "            for doc_no, doc_id in js.items():\n",
    "                if docNo == doc_no:\n",
    "                    return doc_no\n",
    "   \n",
    " \n",
    "\n",
    "    # Return the string DocumentNo of the input integer DocumentID.\n",
    "    def getDocNo(self, docId):\n",
    "        with open(\"docindex.txt\",'r') as file:\n",
    "            doc_index = file.read()\n",
    "            js = json.loads(doc_index)\n",
    "            for doc_no, doc_id in js.items():\n",
    "                if int(docId) == doc_id:\n",
    "                    return doc_no\n",
    " \n",
    "    # Return DF.\n",
    "    def DocFreq(self, token):\n",
    "        self.stemmer = SnowballStemmer(language='english')\n",
    "        token = token.lower()\n",
    "        token = self.stemmer.stem(token)\n",
    "        if token in self.term_js.keys():\n",
    "            freq_list = self.term_js[token].values()\n",
    "            freq = len(freq_list)\n",
    "        else:\n",
    "            freq = 0\n",
    "        return (freq)\n",
    "\n",
    "    # Return the frequency of the token in whole collection/corpus.\n",
    "    def CollectionFreq(self, token):\n",
    "        col_freq=0\n",
    "        self.stemmer = SnowballStemmer(language='english')\n",
    "        token = token.lower()\n",
    "        token = self.stemmer.stem(token)\n",
    "        if token in self.term_js.keys():\n",
    "            freq_list = self.term_js[token].values()\n",
    "            for i in freq_list:\n",
    "                col_freq += i\n",
    "        return (col_freq)\n",
    "\n",
    "    # Return posting list in form of {documentID:frequency}.\n",
    "    def getPostingList(self, token):\n",
    "        doc_freq={}\n",
    "        self.stemmer = SnowballStemmer(language='english')\n",
    "        token = token.lower()\n",
    "        token = self.stemmer.stem(token)\n",
    "        if token in self.term_js.keys():\n",
    "            doc_list = self.term_js[token]\n",
    "            for k,v in doc_list.items():\n",
    "                doc_freq[k]=v\n",
    "        #doc_freq = (doc for term, doc in self.term_js.items() if term == token)\n",
    "        return doc_freq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794e6d9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'guru.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-50186128c668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mMyIndexReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Yahoo\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# retrieve the token.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDocFreq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-679d14e5133a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"guru.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm_js\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'guru.txt'"
     ]
    }
   ],
   "source": [
    "index =MyIndexReader()\n",
    "token = \"Yahoo\"\n",
    "# retrieve the token.\n",
    "df = index.DocFreq(token)\n",
    "print(df)\n",
    "ctf = index.CollectionFreq(token)\n",
    "print(ctf)\n",
    "print(\" >> the token \\\"\"+token+\"\\\" appeared in \"+ str(df) +\" documents and \"+ str(ctf) +\" times in total\")\n",
    "if df>0:\n",
    "    posting = index.getPostingList(token)\n",
    "    for docId in posting:\n",
    "        docNo = index.getDocNo(docId)\n",
    "        print(docNo+\"\\t\"+str(docId)+\"\\t\"+str(posting[docId]))\n",
    "        print(docNo)\n",
    "        print(\"\\n\")\n",
    "        print(docId)\n",
    "        print(\"\\n\")\n",
    "        print(posting[docId])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d535bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocNo(docId):\n",
    "        with open(\"docindex.txt\",'r') as file:\n",
    "            doc_index = file.read()\n",
    "            js = json.loads(doc_index)\n",
    "            for doc_no, doc_id in js.items():\n",
    "                if docId == doc_id:\n",
    "                    print(doc_no,doc_id)\n",
    "                    return doc_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa61750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XIE19960101.0002 2\n",
      "XIE19960101.0002\n"
     ]
    }
   ],
   "source": [
    "doc = getDocNo(2)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73c8b309",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9b8b86f4aa43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mClasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPreprocessedCorpusReader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Classes'"
     ]
    }
   ],
   "source": [
    "import Cla\n",
    "\n",
    "class PreprocessedCorpusReader:\n",
    "\n",
    "    def __init__(self, type):\n",
    "        self.doc = open(Path.ResultHM1 +str(type), 'r')\n",
    "        return\n",
    "\n",
    "    # Read a line for docNo from the corpus, read another line for the content, and return them in [docNo, content].\n",
    "    def nextDocument(self):\n",
    "        doc_no = self.doc.readline().strip()\n",
    "        if(doc_no == \"\"):\n",
    "            self.doc.close()\n",
    "        else:\n",
    "            doc_content = self.doc.readline().strip()\n",
    "            return [doc_no,doc_content]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ba5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for referencing the path of the files\n",
    "import Classes.Path as Path\n",
    "\n",
    "#to write a dictionary in a file\n",
    "import json\n",
    "\n",
    "# Efficiency and memory cost should be paid with extra attention.\n",
    "class MyIndexWriter:\n",
    "    doc_id = 1\n",
    "    term_index={}\n",
    "    doc_index={}\n",
    "    def __init__(self, type):\n",
    "        self.type = type\n",
    "        self.doc = open(Path.ResultHM1 + type, 'r')\n",
    "        return\n",
    "\n",
    "    # This method build index for each document.\n",
    "\t# NT: in your implementation of the index, you should transform your string docno into non-negative integer docids,\n",
    "    # and in MyIndexReader, you should be able to request the integer docid for each docno.\n",
    "    def index(self, docNo, content):\n",
    "        #creating index to keep a record of doc_id and docNo\n",
    "        self.doc_index[docNo] = self.doc_id\n",
    "        \n",
    "        #Creating term index {term:{doc_id:freq}}\n",
    "        doc_content = content.split(\" \")\n",
    "        for i in range(0,len(doc_content)-1):\n",
    "            term = doc_content[i]\n",
    "            \n",
    "            #Case1: term is in Index\n",
    "            if term in self.term_index.keys():\n",
    "                \n",
    "                #Case1.1: term is in Index & Doc_id is in Index\n",
    "                #Action: Increase the term frequency by 1\n",
    "                if self.doc_id in self.term_index[term].keys(): \n",
    "                    self.term_index[term][self.doc_id] += 1\n",
    "                \n",
    "                #Case1.2: term is in Index & Doc_id is not in Index\n",
    "                #Action: Add doc_id with the term frequency set as 1\n",
    "                else:\n",
    "                    self.term_index[term][self.doc_id] = 1   \n",
    "            \n",
    "            #Case2: term is not in the Index\n",
    "            #Action: Add the term in the dictionary with the corresponding doc_id and term frequency set as 1\n",
    "            else:    \n",
    "                self.term_index[term]={}\n",
    "                self.term_index[term][self.doc_id] = 1\n",
    "        \n",
    "        #Incrementing doc_id for the next document\n",
    "        self.doc_id += 1    \n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Close the index writer, and you should output all the buffered content (if any).\n",
    "    def close(self):\n",
    "        \n",
    "        #for writing term index and posting index for the data type trectext\n",
    "        if self.type == 'trectext':\n",
    "            \n",
    "            #Writing Term Index\n",
    "            with open(Path.IndexTextDir+\"TextTermIndex.txt\",'w+') as file:\n",
    "                file.writelines(json.dumps(self.term_index))\n",
    "            \n",
    "            #Writing posting index\n",
    "            with open(Path.IndexTextDir+\"TextDocIndex.txt\",\"w+\") as file1:\n",
    "                file1.writelines(json.dumps(self.doc_index))\n",
    "        \n",
    "        #for writing term index and posting index for the data type trectext\n",
    "        else:\n",
    "            \n",
    "            #Writing Term Index\n",
    "            with open(Path.IndexWebDir+\"WebTermIndex.txt\",'w+') as file:\n",
    "                file.writelines(json.dumps(self.term_index))\n",
    "            \n",
    "            #Writing posting index\n",
    "            with open(Path.IndexWebDir+\"WebDocIndex.txt\",\"w+\") as file1:\n",
    "                file1.writelines(json.dumps(self.doc_index))\n",
    "         \n",
    "        return\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
