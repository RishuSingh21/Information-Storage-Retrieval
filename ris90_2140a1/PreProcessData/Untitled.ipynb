{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c2827bda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-aeea28e4bfe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mbs_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdocno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DOCNO'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'</DOCHDR>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#content = remove_html_tags(con)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "# Reading the data inside the xml file to a variable under the name  data\n",
    "with open('data/docset.trecweb', 'r') as f:\n",
    "    data = f.read() \n",
    "DataList = data.split('</DOC>')\n",
    "#print(DataList[0])\n",
    "for i in range(0,len(DataList)-1):\n",
    "    bs_data = BeautifulSoup(DataList[i],'xml')\n",
    "    docno = bs_data.find('DOCNO')\n",
    "    con = DataList[i].split('</DOCHDR>')[1]\n",
    "    content = BeautifulSoup(con,'xml').text\n",
    "    #content = remove_html_tags(con)\n",
    "    print(content)\n",
    "    #doc1 = bs_data.find('DOCHDR').text\n",
    "    #print(doc1)\n",
    "# Passing the stored data inside the beautifulsoup parser \n",
    "#bs_data = BeautifulSoup(data) \n",
    "#doc = bs_data.find_all('DOCNO')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4ed9956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "class TrectextCollection:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 1. Open the file in Path.DataTextDir.\n",
    "        # 2. Make preparation for function nextDocument().\n",
    "        # NT: you cannot load the whole corpus into memory!!\n",
    "        with open('data/docset.trectext 2', 'r') as f:\n",
    "            data = f.read() \n",
    "        self.dataList = data.split('</DOC>')\n",
    "        self.count=0\n",
    "        return\n",
    "\n",
    "    def nextDocument(self):\n",
    "        # 1. When called, this API processes one document from corpus, and returns its doc number and content.\n",
    "        # 2. When no document left, return null, and close the file.\n",
    "        docNo = \"\"\n",
    "        content = \"\"\n",
    "        bs_soup = BeautifulSoup(self.dataList[self.count],'xml')\n",
    "        if(self.count==len(self.dataList)-1):\n",
    "            return None\n",
    "        docNo = bs_soup.find('DOCNO').text\n",
    "        content = bs_soup.find('TEXT').text\n",
    "        self.count = self.count+1\n",
    "        return [docNo, content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2bfd63e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = TrectextCollection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0e82ac10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The normalization with Egypt, an active supporter of the\n",
      "anti-Iraq coalition during the Gulf crisis, was symbolized by the\n",
      "visit of Egyptian President Hosni Mubarak to Jordan last January.\n",
      "At the same time, Jordan improved its ties with the\n",
      "Palestinians by dispelling their fears that arouse among the\n",
      "Palestinians due to Jordan's special role in Jerusalem under the\n",
      "Jordanian-Israeli accord.\n",
      "In addition, Jordan also made efforts to push forward the\n",
      "Palestinian-Israeli negotiations on expanding Palestinian autonomy\n",
      "in the West Bank.\n",
      "To its own benefits, Jordan's peace treaty with Israel and new\n",
      "attitude toward Baghdad have also brought about warmer ties with\n",
      "the Western countries, with the United States in particular.\n",
      "Among others, U.S. Vice President Al Gore, British Prime\n",
      "Minister John Major, German Chancellor Helmut Kohl, Japanese Prime\n",
      "Minister Tomiichi Murayama as well as Spanish Prime Minister\n",
      "Felipe Gonzalez, visited Amman and reiterated their countries'\n",
      "support for the kingdom in various fields.\n",
      "U.S. Secretary of State Warren Christopher came to Jordan time\n",
      "and again to meet with King Hussein during his more than a dozen\n",
      "of shuttle visits to the region to expedite the ongoing Middle\n",
      "East peace process.\n",
      "The Clinton Administration also voiced its full support, without\n",
      "reservation, to the kingdom to defend its security immediately\n",
      "after Iraqi former Industry Minister Kamel Hassan defected to\n",
      "Jordan in August, warning against a threat of Iraqi retaliation\n",
      "upon Jordan.\n",
      "In the past year, King Hussein, Crown Prince Hassan and Prime\n",
      "Minister Sharif Zeid Ben Shaker as well as other senior Jordanian\n",
      "officials traveled to London, Paris, Tokyo, Bonn and Washington,\n",
      "bringing home agreements for economic aid or military assistance\n",
      "as well as promises of support.\n",
      "Its success of hosting the second Middle East and North Africa\n",
      "economic summit in Amman, which drew the participation of 1,600\n",
      "government officials and private businessmen from 62 countries\n",
      "across the world, added great credit to the kingdom and was considered\n",
      "the crowning accomplishment of Jordan's foreign policy. \n",
      "\n",
      "\n",
      "AMMAN, January 1 (Xinhua) -- Jordan, in the past year,\n",
      "witnessed a series of remarkable achievements on its diplomatic\n",
      "front.\n",
      "In the year, the kingdom restored its territorial and water\n",
      "rights from Israel, normalized relations with the Jewish state,\n",
      "improved ties with other Arab states and moved closer to the\n",
      "Western countries, especially the United States.\n",
      "The historic Israel-Jordan peace treaty signed in October,\n",
      "1994, which ended 46 years of enmity between them, heralded a new\n",
      "era for Jordan's foreign policy and was consequently lauded as a\n",
      "model of peace in the region.\n",
      "In 1995, Jordan regained sovereignty over its territories\n",
      "occupied by Israel and exchanged ambassadors with Tel Aviv. It\n",
      "also signed a number of agreements with Israel, expanding peace to\n",
      "economic cooperation.\n",
      "Its grant of political asylum to General Hussein Kamel Hassan,\n",
      "son-in-law of President Saddam Hussein, and its growing\n",
      "estrangement from Iraq, have quickened the pace to improve its\n",
      "relations with the Gulf Arab states, which were strained because\n",
      "of Jordan's tilt toward Iraq during the 1990-1991 Gulf crisis.\n",
      "In the past year, the kingdom also saw an outstanding warm-up\n",
      "of ties with Saudi Arabia, the strongest of the six-nation Gulf\n",
      "Cooperation Council (GCC).\n",
      "Jordan's Foreign Minister Abdul-Karim Kabariti visited Riyadh\n",
      "twice during the year and was received by Saudi King Fahd on his\n",
      "second visit to the oil-rich Gulf kingdom.\n",
      "Saudi Ambassador to Jordan Abdullah Sudairi arrived in Amman\n",
      "last month and took his post five years after Saudi Arabia\n",
      "recalled its ambassador in Amman, marking the return to normal of\n",
      "the bilateral relations.\n",
      "A visit to Saudi Arabia by King Hussein and his summit meeting\n",
      "with King Fahd, which had been scheduled for early this month but\n",
      "were canceled due to Fahd's sudden illness, are expected to\n",
      "materialize soon.\n",
      "Kuwait, invaded by Iraq in 1990, has not formally announced its\n",
      "normalization with Jordan, but has been moving closer to it.\n",
      "The two countries' foreign ministers met repeatedly on the\n",
      "sidelines of many regional and international gatherings and\n",
      "agreed to conducting regular coordination and consultation.\n",
      "\n",
      "\n",
      "CAIRO, January 1 (Xinhua) -- Violence seems on the rise in\n",
      "Egypt as a latest sign that militant attacks may be intensified in\n",
      "the most populous Arab country.\n",
      "The latest violence took place today in southern Egypt where\n",
      "three policemen were slightly injured by unidentified gunmen in\n",
      "southern Egypt.\n",
      "The official Middle East News Agency (MENA) quoted a security\n",
      "source as saying that the gunmen opened fire at a police car,\n",
      "which was passing near plantations close to the al-Rodah village\n",
      "in Malawi, Minya, 260 kilometers south of Cairo.\n",
      "The car, carrying license plates of the New Valley Security\n",
      "Directorate, was on its way to Cairo when being attacked,\n",
      "according to MENA. The security forces launched an operation\n",
      "trying to arrest the gunmen.\n",
      "It was on the second consecutive day that such a militant\n",
      "attack took place in Minya.\n",
      "On Sunday, unidentified assailants killed two policemen and one\n",
      "civilian as they opened fire at a traffic police patrol car on a\n",
      "road west of Abu Korkas in the governorate of Minya.\n",
      "Four other policemen and as many civilians, including a young\n",
      "girl, were injured during the attack.\n",
      "Last Thursday, three gunmen killed one person and injured\n",
      "another in Abu Korkas.\n",
      "Egypt has been haunted by terrorist attacks, especially in the\n",
      "south, since 1992. Most of these attacks were believed to have\n",
      "been carried out by Islamic militants who seek to topple\n",
      "toppling the secular government and replacing it with a strict\n",
      "Islamic rule. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    doc = collection.nextDocument()\n",
    "    if(doc==None):\n",
    "        break\n",
    "    print(doc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c115310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "class WordNormalizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        #stemmer requires a language parameter\n",
    "        self.stemmer = SnowballStemmer(language='english')\n",
    "        return\n",
    "\n",
    "    def lowercase(self, word):\n",
    "        # Transform the word uppercase characters into lowercase.\n",
    "        word = word.lower()\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        # Return the stemmed word with Stemmer in Classes package.\n",
    "        word = self.stemmer.stem(word)\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05ec87b0",
   "metadata": {},
   "outputs": [],
   "source": [
    " w = WordNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61912751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'youy'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.lowercase('YOuy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88f5f1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mape'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.stem('Maping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc4524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "# Efficiency and memory cost should be paid with extra attention.\n",
    "# Essential private methods or variables can be added.\n",
    "\n",
    "\n",
    "class WordTokenizer:\n",
    "    # per TA feedback, manually tokenize by keeping only letter expressions\n",
    "    regex = re.compile('[a-zA-Z]+')\n",
    "\n",
    "    def __init__(self, content: str):\n",
    "        # Tokenize the input texts.\n",
    "\n",
    "        # tracking our position inside the tokens so we can return None when we're done\n",
    "        self.index = 0\n",
    "        self.tokenized_content = WordTokenizer.regex.findall(content)\n",
    "\n",
    "        return\n",
    "\n",
    "    def nextWord(self):\n",
    "        # Return the next word in the document.\n",
    "        # Return null, if it is the end of the document.\n",
    "\n",
    "        # increment the index\n",
    "        self.index += 1\n",
    "\n",
    "        # return None if we're out of bounds\n",
    "        if self.index >= len(self.tokenized_content):\n",
    "            return None\n",
    "\n",
    "        # else return the token\n",
    "        return self.tokenized_content[self.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f0996b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u\n"
     ]
    }
   ],
   "source": [
    "t=WordTokenizer('o*&u red')\n",
    "u = t.nextWord()\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62a03db5",
   "metadata": {},
   "outputs": [],
   "source": [
    " def token(content):\n",
    "        # Tokenize the input texts.\n",
    "        content = re.findall('[a-zA-Z]+',content)\n",
    "        \n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ec848d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', 'u', 'r', 'e', 'd', 'eioerhueghvfhjdb']\n"
     ]
    }
   ],
   "source": [
    "word = token('o$u r*e0 d 2eioerhueghvfhjdb')\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "540cf319",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordTokenizer:\n",
    "\n",
    "    def __init__(self, content):\n",
    "        # Tokenize the input texts.\n",
    "        self.content = re.findall('[a-zA-Z]+',content)\n",
    "        self.count=0\n",
    "        return\n",
    "\n",
    "    def nextWord(self):\n",
    "        # Return the next word in the document.\n",
    "        # Return null, if it is the end of the document.\n",
    "        self.count = self.count+1\n",
    "        word = \"\"\n",
    "        if(self.count >= len(self.content)):\n",
    "            return None\n",
    "        else:\n",
    "            word = self.content[self.count]\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "23655529",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = WordTokenizer('ohre@d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b3e09073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": [
    "print(word.nextWord())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd24a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 1 docs\n"
     ]
    }
   ],
   "source": [
    "count=1\n",
    "print(\"finish \" + count + \" docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a3937b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
